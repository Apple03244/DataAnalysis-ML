{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GD(gradient descent)\n",
    "비용함수를 최소화하기 위해 반복하는 파라미터를 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta$에 대한 비용함수의 `gradient`가 감소하는 방향으로 $\\theta'$를 수정\n",
    "\n",
    "이때, 랜덤초기화(즉 임의의 값 $\\theta$에서 시작)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 step의 크기를 `학습률`파라미터로 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 참고          \n",
    "`전역 최솟값`을 찾는게 베스트.          \n",
    "`지역 최솟값`을 찾았다고 `전역 최솟값`을 찾았다고 판단하지 않게 주의            \n",
    "`parameter space`에서 최적의 해를 찾아야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1. 배치 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하나의 $\\theta_j \\in \\Theta$(parameter space) 에 대한 `residual deriative`는 아래와 같다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial}{\\partial \\theta_j} MSE(\\theta)=\\frac{2}{m}\\sum_{j=1}^{m}(\\theta^T x^i - y^i)x_j^i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 우리는 아래의 그래디언트 벡터 $\\nabla_\\theta MSE(\\theta) 를 고려할 수 있다$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_\\theta MSE=(\\frac{\\partial}{\\partial \\theta_1} MSE(\\theta),..,\\frac{\\partial}{\\partial \\theta_n} MSE(\\theta))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "책의 설명과 다르게 이해하자면 n차원에서의 미분가능성을 가정한다면, 우리는 optimization problem을 풀기 위해 위의 그래디언트 값이 0에 수렴해야한다.\n",
    "따라서 $\\theta$값의 변화를 주어야하는데 그래디언트 값이 0으로 수렴시키기 위해 아래의 값을 고려한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta^{(next-step)}=\\theta-\\eta \\nabla_\\theta MSE(\\theta)$, where $\\eta$ =학습률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2. 확률적 경사 하강법(SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 parameter search 방법이 좋다는 것을 이해하고 난 뒤, 우리는 계산량이 굉장히 많다는 것을 알 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매 스텝헤서 한 개의 랜덤샘플을 통해 그래디언트를 계산하고 계산량을 크게 줄일 수 있지만, 문제는 그래디언트가 요동치므로 최적값에 도착할 수 없다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 요동치므로 `지역최솟값`을 건너뛸 수 있는 장점이 있는데 알고리즘이 `전역최솟값`에 도달할 수 있게 학습률을 점진적으로 감소시켜야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매 반복마다 학습률을 계산하는 함수를 `learning schedule`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 한 반복에서 되풀이되는 횟수를 `epoch`(에포크)라고 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
